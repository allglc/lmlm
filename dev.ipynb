{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 32\n",
    "batch_size = 256\n",
    "nb_iters = 10000\n",
    "eval_iters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt monseigneur Bienvenu   Le palais épiscopal de Digne était attenant à l'hôpital.  Le palais épiscopal était un vaste et bel hôtel bâti en pierre au commencement du siècle dernier par monseigneur Henri Puget, docteur en théologie de la faculté de Paris, abbé de Simore, lequel était évêque de Digne en 1712. Ce palais était un vrai logis seigneurial. Tout y avait grand air, les appartements de l'évêque, les salons, les chambres, la cour d'honneur, fort large, avec promenoirs à arcades, selon l'an\n"
     ]
    }
   ],
   "source": [
    "url_tomes = [\n",
    "    'https://www.gutenberg.org/ebooks/17489.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/17493.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/17494.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/17518.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/17519.txt.utf-8'\n",
    "    ]\n",
    "les_miserables = ''\n",
    "for url in url_tomes:\n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'utf-8-sig'\n",
    "    tome = response.text\n",
    "    tome = tome.replace('\\r\\n', ' ')\n",
    "    les_miserables += tome\n",
    "print(les_miserables[10000:10500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 characters:\n",
      " !\"#$%'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz«°º»ÀÂÇÈÉÊÔàâæçèéêëîïñôöùûü—‘’“”•™\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(set(les_miserables))\n",
    "vocab_size = len(characters)\n",
    "print(f\"{vocab_size} characters:\\n{''.join(characters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"#$%'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz«°º»ÀÂÇÈÉÊÔàâæçèéêëîïñôöùûü—‘’“”•™\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]\n",
      " !\"#$%'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz«°º»ÀÂÇÈÉÊÔàâæçèéêëîïñôöùûü—‘’“”•™\n"
     ]
    }
   ],
   "source": [
    "char_to_int = {c: i for i, c in enumerate(characters)}\n",
    "encode = lambda all_c: [char_to_int[c] for c in all_c]\n",
    "int_to_char = {i: c for i, c in enumerate(characters)}\n",
    "decode = lambda all_i: ''.join([int_to_char[i] for i in all_i])\n",
    "\n",
    "print(''.join(characters))\n",
    "print(encode(characters))\n",
    "print(decode(encode(characters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"ous avons à raconter, il n'est p\"]\n",
      "[\"us avons à raconter, il n'est pe\"]\n"
     ]
    }
   ],
   "source": [
    "class LesMiserablesDataset(Dataset):\n",
    "    def __init__(self, str_data, seq_len):\n",
    "        self.data = torch.tensor(encode(str_data))\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.data[idx:idx+self.seq_len]\n",
    "        target = self.data[idx+1:idx+self.seq_len+1]\n",
    "        return context, target\n",
    "\n",
    "train_dataset = LesMiserablesDataset(les_miserables[:int(0.9*len(les_miserables))], seq_len)\n",
    "test_dataset = LesMiserablesDataset(les_miserables[int(0.9*len(les_miserables)):], seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print([decode([i.item() for i in train_dataset[4356][0]])])\n",
    "print([decode([i.item() for i in train_dataset[4356][1]])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = {'train': torch.zeros(eval_iters), 'test': torch.zeros(eval_iters)}\n",
    "    for split, dataloader in zip(['train', 'test'], [train_dataloader, test_dataloader]):\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            y_pred = model(x)\n",
    "            loss = F.cross_entropy(y_pred.view(-1, vocab_size), y.view(-1))\n",
    "            losses[split][i] = loss.item()\n",
    "            if i >= eval_iters-1:\n",
    "                break\n",
    "    losses = {split: losses[split].mean() for split in ['train', 'test']}\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, seq_len, embed_size, nb_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.nb_heads = nb_heads\n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(embed_size, nb_heads*head_size, bias=False)\n",
    "        self.key = nn.Linear(embed_size, nb_heads*head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, nb_heads*head_size, bias=False)\n",
    "        self.mask = (torch.tril(torch.ones(seq_len, seq_len)) == 0)\n",
    "        self.projection = nn.Linear(nb_heads*head_size, embed_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape # x: batch_size x seq_len x embed_size\n",
    "        # compute q, k, v\n",
    "        q = self.query(x) # batch_size x seq_len x nb_heads*head_size\n",
    "        q = q.view(batch_size, seq_len, self.nb_heads, self.head_size) # batch_size x seq_len x nb_heads x head_size\n",
    "        q = q.permute(0, 2, 1, 3) # batch_size x nb_heads x seq_len x head_size\n",
    "        k = self.key(x).view(batch_size, seq_len, self.nb_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.nb_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # compute multi-head attention\n",
    "        att = q @ k.transpose(2, 3) / self.head_size**0.5 # batch_size x nb_heads x seq_len x seq_len\n",
    "        att.masked_fill_(mask=self.mask[:seq_len, :seq_len], value=float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = att @ v # batch_size x nb_heads x seq_len x head_size\n",
    "        \n",
    "        # concatenate heads and project\n",
    "        att = att.permute(0, 2, 1, 3).reshape(batch_size, seq_len, self.nb_heads*self.head_size) # batch_size x seq_len x nb_heads*head_size\n",
    "        att = self.projection(att) # batch_size x seq_len x embed_size\n",
    "        return att\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, seq_len, embed_size, nb_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.masked_multi_head_attention = MaskedMultiHeadAttention(seq_len, embed_size, nb_heads, head_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embed_size, embed_size))\n",
    "        self.layer_norm = nn.LayerNorm(embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.masked_multi_head_attention(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class LesMiserablesLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, embed_size, nb_heads, head_size, n_blocks):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block(seq_len, embed_size, nb_heads, head_size) for _ in range(n_blocks)])\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.token_embedding(x) + self.position_embedding(torch.arange(seq_len)) # batch_size x seq_len x embedding_dim\n",
    "        x = self.blocks(x) # batch_size x seq_len x embedding_dim\n",
    "        x = self.linear(x) # batch_size x seq_len x vocab_size\n",
    "        return x\n",
    "    \n",
    "    def generate(self, x, nb_tokens):\n",
    "        if type(x) == str:\n",
    "            x = torch.tensor(encode(x)).unsqueeze(0)\n",
    "        for _ in range(nb_tokens):\n",
    "            logits = self(x[:, -self.seq_len:])\n",
    "            logits = logits[:, -1, :]\n",
    "            probas = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probas, 1)\n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "        txt = decode(x.tolist()[0])\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss = 4.450, test loss = 4.456, generated text = JeansÇ:kF°?à'_Boo™Læñ#H ï°ö_jAj)*aöm':#«ññ8ñ+IôS-»s4SR\n",
      "iter 500: train loss = 1.849, test loss = 1.863, generated text = Jean pivitisté ses vie.  était-à pour aformante, un pe\n",
      "iter 1000: train loss = 1.640, test loss = 1.642, generated text = Jean Valjean était dans la fhaume un gage entendant en\n",
      "iter 1500: train loss = 1.546, test loss = 1.534, generated text = Jean IV ête au coups de me voulu des aux aternicares.s\n",
      "iter 2000: train loss = 1.471, test loss = 1.492, generated text = Jean Van jamais, les archés approchants:  --Et jamaçon\n",
      "iter 2500: train loss = 1.449, test loss = 1.460, generated text = Jean Val, c'est se grise qui avait eu rentent qu'un qu\n",
      "iter 3000: train loss = 1.432, test loss = 1.446, generated text = Jean Valjean qui sentait auxisités carrés dans les mes\n",
      "iter 3500: train loss = 1.402, test loss = 1.420, generated text = Jean Valjean, et croisant traversa la garde le frère e\n",
      "iter 4000: train loss = 1.387, test loss = 1.401, generated text = Jean Valjean s'était soir la sura son poénart.  Il y a\n",
      "iter 4500: train loss = 1.383, test loss = 1.403, generated text = Jean Valjean regard à quatentions s'informaient qu'il \n",
      "iter 5000: train loss = 1.356, test loss = 1.369, generated text = Jean ma forme et to andeau était près de la surginèbre\n",
      "iter 5500: train loss = 1.355, test loss = 1.364, generated text = Jean-Ton peut été feu! Où Alors dérésespérait-ces mêlé\n",
      "iter 6000: train loss = 1.334, test loss = 1.360, generated text = Jean Valjean! C'était Pieds dors la talue de ferma cha\n",
      "iter 6500: train loss = 1.337, test loss = 1.361, generated text = Jean Valjean; immobile à la peine, ces canaît tout cel\n",
      "iter 7000: train loss = 1.330, test loss = 1.341, generated text = Jean Valjean y fait, il leva un peu pouvoir où les dét\n",
      "iter 7500: train loss = 1.332, test loss = 1.332, generated text = Jean Valjean. On se sentait avec une forêt adroirée do\n",
      "iter 8000: train loss = 1.312, test loss = 1.335, generated text = Jean Valjean s'y avait ce tupérie, il fit invultifs? P\n",
      "iter 8500: train loss = 1.304, test loss = 1.333, generated text = Jean Valjean le sifcrax, et la quinzeur_. D'un géniers\n",
      "iter 9000: train loss = 1.308, test loss = 1.320, generated text = Jean Valjean, la Saint-Malheur et la main marronnant e\n",
      "iter 9500: train loss = 1.299, test loss = 1.318, generated text = Jean Valjean saisit à se un petit égal de gredoupe. Da\n"
     ]
    }
   ],
   "source": [
    "model = LesMiserablesLanguageModel(vocab_size, seq_len, embed_size=64, nb_heads=4, head_size=16, n_blocks=6)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "for iter, (x, y) in enumerate(train_dataloader):\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = F.cross_entropy(y_pred.view(-1, vocab_size), y.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if iter % 500 == 0:\n",
    "        losses = evaluate()\n",
    "        print(f\"iter {iter}: train loss = {losses['train'].item():.3f}, test loss = {losses['test'].item():.3f}, generated text = {model.generate('Jean', 50)}\")\n",
    "        \n",
    "    if iter >= nb_iters-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
