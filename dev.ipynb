{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt monseigneur Bienvenu   Le palais épiscopal de Digne était attenant à l'hôpital.  Le palais épiscopal était un vaste et bel hôtel bâti en pierre au commencement du siècle dernier par monseigneur Henri Puget, docteur en théologie de la faculté de Paris, abbé de Simore, lequel était évêque de Digne en 1712. Ce palais était un vrai logis seigneurial. Tout y avait grand air, les appartements de l'évêque, les salons, les chambres, la cour d'honneur, fort large, avec promenoirs à arcades, selon l'an\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./les_miserables.txt'):\n",
    "    with open('./les_miserables.txt', 'r', encoding='utf-8-sig') as file:\n",
    "        les_miserables = file.read()\n",
    "else:\n",
    "    url_tomes = [\n",
    "        'https://www.gutenberg.org/ebooks/17489.txt.utf-8',\n",
    "        'https://www.gutenberg.org/ebooks/17493.txt.utf-8',\n",
    "        'https://www.gutenberg.org/ebooks/17494.txt.utf-8',\n",
    "        'https://www.gutenberg.org/ebooks/17518.txt.utf-8',\n",
    "        'https://www.gutenberg.org/ebooks/17519.txt.utf-8'\n",
    "        ]\n",
    "    les_miserables = ''\n",
    "    for url in url_tomes:\n",
    "        response = requests.get(url)\n",
    "        response.encoding = 'utf-8-sig'\n",
    "        tome = response.text\n",
    "        tome = tome.replace('\\r\\n', ' ')\n",
    "        les_miserables += tome\n",
    "    with open('./les_miserables.txt', 'w') as file:\n",
    "        file.write(les_miserables)\n",
    "        \n",
    "print(les_miserables[10000:10500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character level tokenizer\n",
      "Example text:  !\"#$%'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz«°º»ÀÂÇÈÉÊÔàâæçèéêëîïñôöùûü—‘’“”•™\n",
      "Encoded: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]\n",
      "Encoded+decoded:  !\"#$%'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz«°º»ÀÂÇÈÉÊÔàâæçèéêëîïñôöùûü—‘’“”•™\n",
      "\n",
      "Byte Pair Encoding tokenizer\n",
      "Example text: ns çà et là dans l'azur pâle et profond, la terre toute noire, le ciel tout blanc, un frisson dans l\n",
      "Encoded: [110, 257, 529, 291, 283, 651, 821, 97, 122, 117, 271, 112, 481, 275, 283, 470, 738, 100, 260, 284, 550, 276, 819, 110, 279, 462, 275, 446, 426, 485, 486, 268, 99, 260, 314, 102, 300, 319, 299, 337, 108]\n",
      "Encoded+decoded: ns çà et là dans l'azur pâle et profond, la terre toute noire, le ciel tout blanc, un frisson dans l\n"
     ]
    }
   ],
   "source": [
    "class CharacterLevelTokenizer:\n",
    "    def __init__(self, train_text):\n",
    "        self.train(train_text)\n",
    "   \n",
    "    def train(self, train_text):\n",
    "        self.characters = sorted(set(train_text))\n",
    "        self.vocab_size = len(self.characters)\n",
    "        self.char_to_int = {c: i for i, c in enumerate(self.characters)}\n",
    "        self.int_to_char = {i: c for i, c in enumerate(self.characters)}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        return [self.char_to_int[c] for c in text]\n",
    "    \n",
    "    def decode(self, text):\n",
    "        return ''.join([self.int_to_char[i] for i in text])\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, train_text, vocab_size):\n",
    "        self.train(train_text, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def _replace_pair(self, tokens, pair, idx):\n",
    "            tokens_bpe = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                    tokens_bpe.append(idx)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    tokens_bpe.append(tokens[i])\n",
    "                    i += 1\n",
    "            return tokens_bpe\n",
    "        \n",
    "    def train(self, train_text, vocab_size):\n",
    "        tokens = list(train_text.encode('utf-8'))\n",
    "        self.merges = {}\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n",
    "        for idx in range(256, vocab_size):\n",
    "            # get most frequent pair\n",
    "            pair_counts = {}\n",
    "            for pair in zip(tokens[:-1], tokens[1:]):\n",
    "                pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "            max_pair = max(pair_counts, key=pair_counts.get)\n",
    "            \n",
    "            # save results            \n",
    "            self.merges[max_pair] = idx\n",
    "            self.vocab[idx] = self.vocab[max_pair[0]] + self.vocab[max_pair[1]]\n",
    "            \n",
    "            # replace most frequent pair by new idx, others stay the same\n",
    "            tokens = self._replace_pair(tokens, max_pair, idx)\n",
    "            \n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        for pair, idx in self.merges.items():\n",
    "            tokens = self._replace_pair(tokens, pair, idx)\n",
    "        return tokens\n",
    "        \n",
    "    def decode(self, tokens):\n",
    "        text_bytes = b''.join([self.vocab[t] for t in tokens])\n",
    "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\n",
    "\n",
    "print('Character level tokenizer')\n",
    "tokenizer = CharacterLevelTokenizer(les_miserables)\n",
    "example_text = ''.join(tokenizer.characters)\n",
    "print('Example text:', example_text)\n",
    "print('Encoded:', tokenizer.encode(example_text))\n",
    "print('Encoded+decoded:', tokenizer.decode(tokenizer.encode(example_text)))\n",
    "\n",
    "print('\\nByte Pair Encoding tokenizer')\n",
    "tokenizer = BPETokenizer(les_miserables[:100000], vocab_size=1000)\n",
    "example_text = les_miserables[2000000:2000100]\n",
    "print('Example text:', example_text)\n",
    "print('Encoded:', tokenizer.encode(example_text))\n",
    "print('Encoded+decoded:', tokenizer.decode(tokenizer.encode(example_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: ['Monsieur M']\n",
      "y: ['onsieur My']\n"
     ]
    }
   ],
   "source": [
    "class LesMiserablesDataset(Dataset):\n",
    "    def __init__(self, str_data, seq_len, tokenizer):\n",
    "        self.data = torch.tensor(tokenizer.encode(str_data))\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.data[idx:idx+self.seq_len]\n",
    "        target = self.data[idx+1:idx+self.seq_len+1]\n",
    "        return context, target\n",
    "\n",
    "tokenizer = CharacterLevelTokenizer(les_miserables)\n",
    "example_dataset = LesMiserablesDataset(les_miserables[:10000], 10, tokenizer)\n",
    "\n",
    "print('x:', [tokenizer.decode([i.item() for i in example_dataset[1000][0]])])\n",
    "print('y:', [tokenizer.decode([i.item() for i in example_dataset[1000][1]])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, seq_len, embed_size, nb_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.nb_heads = nb_heads\n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(embed_size, nb_heads*head_size, bias=False)\n",
    "        self.key = nn.Linear(embed_size, nb_heads*head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, nb_heads*head_size, bias=False)\n",
    "        self.projection = nn.Linear(nb_heads*head_size, embed_size)\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(seq_len, seq_len)) == 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape # x: batch_size x seq_len x embed_size\n",
    "        # compute q, k, v\n",
    "        q = self.query(x) # batch_size x seq_len x nb_heads*head_size\n",
    "        q = q.view(batch_size, seq_len, self.nb_heads, self.head_size) # batch_size x seq_len x nb_heads x head_size\n",
    "        q = q.permute(0, 2, 1, 3) # batch_size x nb_heads x seq_len x head_size\n",
    "        k = self.key(x).view(batch_size, seq_len, self.nb_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.nb_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # compute multi-head attention\n",
    "        att = q @ k.transpose(2, 3) / self.head_size**0.5 # batch_size x nb_heads x seq_len x seq_len\n",
    "        att.masked_fill_(mask=self.mask[:seq_len, :seq_len], value=float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = att @ v # batch_size x nb_heads x seq_len x head_size\n",
    "        \n",
    "        # concatenate heads and project\n",
    "        att = att.permute(0, 2, 1, 3).reshape(batch_size, seq_len, self.nb_heads*self.head_size) # batch_size x seq_len x nb_heads*head_size\n",
    "        att = self.projection(att) # batch_size x seq_len x embed_size\n",
    "        return att\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, seq_len, embed_size, nb_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.masked_multi_head_attention = MaskedMultiHeadAttention(seq_len, embed_size, nb_heads, head_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embed_size, embed_size))\n",
    "        self.layer_norm = nn.LayerNorm(embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.masked_multi_head_attention(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class LesMiserablesLanguageModel(nn.Module):\n",
    "    def __init__(self, tokenizer, seq_len, embed_size, nb_heads, head_size, n_blocks):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_embedding = nn.Embedding(tokenizer.vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block(seq_len, embed_size, nb_heads, head_size) for _ in range(n_blocks)])\n",
    "        self.linear = nn.Linear(embed_size, tokenizer.vocab_size)\n",
    "        self.seq_len = seq_len\n",
    "        self.register_buffer('seq_arange', torch.arange(seq_len))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.token_embedding(x) + self.position_embedding(self.seq_arange[:seq_len]) # batch_size x seq_len x embedding_dim\n",
    "        x = self.blocks(x) # batch_size x seq_len x embedding_dim\n",
    "        x = self.linear(x) # batch_size x seq_len x vocab_size\n",
    "        return x\n",
    "    \n",
    "    def generate(self, x, nb_tokens):\n",
    "        assert type(x) == str\n",
    "        x = torch.tensor(self.tokenizer.encode(x), device=self.seq_arange.device).unsqueeze(0)\n",
    "        for _ in range(nb_tokens):\n",
    "            logits = self(x[:, -self.seq_len:])\n",
    "            logits = logits[:, -1, :]\n",
    "            probas = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probas, 1)\n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "        txt = self.tokenizer.decode(x.tolist()[0])\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    itr = 0\n",
    "    while itr < nb_iters: # otherwise dataloader ends the training when all dataset iterated\n",
    "        for x, y in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = F.cross_entropy(y_pred.view(-1, tokenizer.vocab_size), y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if itr % 2000 == 0:\n",
    "                losses = evaluate()\n",
    "                print(f\"iter {itr}: train loss = {losses['train'].item():.3f}, test loss = {losses['test'].item():.3f}, generated text = {model.generate('Jean', 30)}\")\n",
    "            if itr >= nb_iters:\n",
    "                break\n",
    "            \n",
    "            itr += 1\n",
    "            \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = {'train': torch.zeros(eval_iters), 'test': torch.zeros(eval_iters)}\n",
    "    for split, dataloader in zip(['train', 'test'], [train_dataloader, test_dataloader]):\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = F.cross_entropy(y_pred.view(-1, tokenizer.vocab_size), y.view(-1))\n",
    "            losses[split][i] = loss.item()\n",
    "            if i >= eval_iters-1:\n",
    "                break\n",
    "    losses = {split: losses[split].mean() for split in ['train', 'test']}\n",
    "    model.train()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss = 6.960, test loss = 6.958, generated text = Jeanademoisdu Kmaiferementmane \u0004toute anlaissation �kaissmarest ez-livres_ _�\tour �opi�bre ��\n",
      "iter 2000: train loss = 3.289, test loss = 3.294, generated text = Jean Valjean, par le fait de la vierge inthe Jonffle jusqu'avait une gouche \n",
      "iter 4000: train loss = 3.045, test loss = 3.086, generated text = Jean Valjean, auprès d'y seulement trois cent mille trois hommes.  Aucun crime.     Chapitre X\n",
      "iter 6000: train loss = 2.954, test loss = 3.023, generated text = Jean Valjean se comprit.  Il recommenait Muphant. Toute de Marius et D\n",
      "iter 8000: train loss = 2.882, test loss = 3.001, generated text = Jean Valjean tournait main. C'était la longue figure sortait qu'elle s'abaissa \n",
      "iter 10000: train loss = 2.862, test loss = 2.963, generated text = Jean Valjean chez lui, avec une sortie qui perverne à aimer le désinsi st\n",
      "iter 12000: train loss = 2.828, test loss = 2.941, generated text = Jean Valjean alors, artillé se retenant des coudes aux mères, entendez-moi\n",
      "iter 14000: train loss = 2.793, test loss = 2.923, generated text = Jean-Jacquin un âge et noir, puis il est beaucoup moins de la ville; mame Blü\n",
      "iter 16000: train loss = 2.794, test loss = 2.928, generated text = Jean Grèce, un visage d'éduce, de la ruelle, une heure de statue et é\n",
      "iter 18000: train loss = 2.760, test loss = 2.918, generated text = Jean Valjean s'y sentit de l'enfant, avait faire eu quelque chose châtier escalader \n",
      "iter 20000: train loss = 2.756, test loss = 2.919, generated text = Jean Valjean. Il n'y avait qu'à cet issue qui y voulait rendre tout ce que M. Leblanc\n"
     ]
    }
   ],
   "source": [
    "seq_len = 64\n",
    "vocab_size = 1000\n",
    "batch_size = 256\n",
    "nb_iters = 20000\n",
    "eval_iters = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# tokenizer = CharacterLevelTokenizer(train_text=les_miserables)\n",
    "tokenizer = BPETokenizer(train_text=les_miserables[:100000], vocab_size=vocab_size)\n",
    "\n",
    "train_dataset = LesMiserablesDataset(les_miserables[:int(0.9*len(les_miserables))], seq_len, tokenizer)\n",
    "test_dataset = LesMiserablesDataset(les_miserables[int(0.9*len(les_miserables)):], seq_len, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = LesMiserablesLanguageModel(tokenizer, seq_len, embed_size=64, nb_heads=4, head_size=16, n_blocks=6).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.2.0_py3.11.7",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.2.0_py3.11.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
