{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 32\n",
    "batch_size = 128\n",
    "nb_iters = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt monseigneur Bienvenu   Le palais épiscopal de Digne était attenant à l'hôpital.  Le palais épiscopal était un vaste et bel hôtel bâti en pierre au commencement du siècle dernier par monseigneur Henri Puget, docteur en théologie de la faculté de Paris, abbé de Simore, lequel était évêque de Digne en 1712. Ce palais était un vrai logis seigneurial. Tout y avait grand air, les appartements de l'évêque, les salons, les chambres, la cour d'honneur, fort large, avec promenoirs à arcades, selon l'an\n"
     ]
    }
   ],
   "source": [
    "url_tomes = [\n",
    "    'https://www.gutenberg.org/ebooks/17489.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/17493.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/17494.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/17518.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/17519.txt.utf-8'\n",
    "    ]\n",
    "les_miserables = ''\n",
    "for url in url_tomes:\n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'utf-8-sig'\n",
    "    tome = response.text\n",
    "    tome = tome.replace('\\r\\n', ' ')\n",
    "    les_miserables += tome\n",
    "print(les_miserables[10000:10500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 characters:\n",
      " !\"#$%'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz«°º»ÀÂÇÈÉÊÔàâæçèéêëîïñôöùûü—‘’“”•™\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(set(les_miserables))\n",
    "vocab_size = len(characters)\n",
    "print(f\"{vocab_size} characters:\\n{''.join(characters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"#$%'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz«°º»ÀÂÇÈÉÊÔàâæçèéêëîïñôöùûü—‘’“”•™\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]\n",
      " !\"#$%'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz«°º»ÀÂÇÈÉÊÔàâæçèéêëîïñôöùûü—‘’“”•™\n"
     ]
    }
   ],
   "source": [
    "char_to_int = {c: i for i, c in enumerate(characters)}\n",
    "encode = lambda all_c: [char_to_int[c] for c in all_c]\n",
    "int_to_char = {i: c for i, c in enumerate(characters)}\n",
    "decode = lambda all_i: ''.join([int_to_char[i] for i in all_i])\n",
    "\n",
    "print(''.join(characters))\n",
    "print(encode(characters))\n",
    "print(decode(encode(characters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"ous avons à raconter, il n'est p\"]\n",
      "[\"us avons à raconter, il n'est pe\"]\n"
     ]
    }
   ],
   "source": [
    "class LesMiserablesDataset(Dataset):\n",
    "    def __init__(self, str_data, seq_len):\n",
    "        self.data = torch.tensor(encode(str_data))\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.data[idx:idx+self.seq_len]\n",
    "        target = self.data[idx+1:idx+self.seq_len+1]\n",
    "        return context, target\n",
    "\n",
    "train_dataset = LesMiserablesDataset(les_miserables[:int(0.9*len(les_miserables))], seq_len)\n",
    "test_dataset = LesMiserablesDataset(les_miserables[int(0.9*len(les_miserables)):], seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print([decode([i.item() for i in train_dataset[4356][0]])])\n",
    "print([decode([i.item() for i in train_dataset[4356][1]])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLPBaseline(torch.nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "#         super(MLPBaseline, self).__init__()\n",
    "#         self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.fc1 = torch.nn.Linear(embedding_dim, hidden_dim)\n",
    "#         self.fc2 = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x) # batch_size x seq_len x embedding_dim\n",
    "#         x = x.view(x.size(0)*x.size(1), x.size(2))\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         logits = self.fc2(x)\n",
    "#         return logits\n",
    "    \n",
    "# model = MLPBaseline(vocab_size, 64, 128)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# for iter, (x, y) in enumerate(train_dataloader):\n",
    "    \n",
    "#     y_pred = model(x)\n",
    "#     loss = F.cross_entropy(y_pred, y.view(-1))\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     print(loss)\n",
    "    \n",
    "#     if iter >= nb_iters-1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B,T,C = 1,8,32 # batch, time, channels\n",
    "# x = torch.randn(B,T,C)\n",
    "# head_size = 10\n",
    "# query = nn.Linear(C, head_size, bias=False)\n",
    "# key = nn.Linear(C, head_size, bias=False)\n",
    "# value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# q = query(x)\n",
    "# k = key(x)\n",
    "# v = value(x)\n",
    "\n",
    "# att = q @ k.transpose(1, 2)\n",
    "# att = att / head_size**0.5\n",
    "# att.masked_fill_(mask=torch.tril(torch.ones(T, T)) == 0, value=float('-inf'))\n",
    "# att = F.softmax(att, dim=-1)\n",
    "# att = att @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Head(nn.Module):\n",
    "#     def __init__(self, seq_len, embed_size, head_size):\n",
    "#         super().__init__()\n",
    "#         self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "#         self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "#         self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "#         self.mask = (torch.tril(torch.ones(seq_len, seq_len)) == 0)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         q = query(x)\n",
    "#         k = key(x)\n",
    "#         v = value(x)\n",
    "#         att = q @ k.transpose(1, 2) / head_size**0.5\n",
    "#         att.masked_fill_(mask=self.mask, value=float('-inf'))\n",
    "#         att = F.softmax(att, dim=-1)\n",
    "#         att = att @ v\n",
    "#         return att\n",
    "\n",
    "# B,T,C = 3,8,32 # batch, time, channels\n",
    "# head_size = 10\n",
    "# x = torch.randn(B,T,C)  \n",
    "# h = Head(T, C, head_size)\n",
    "# h(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 117])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, seq_len, embed_size, nb_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        self.nb_heads = nb_heads\n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(embed_size, nb_heads*head_size, bias=False)\n",
    "        self.key = nn.Linear(embed_size, nb_heads*head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, nb_heads*head_size, bias=False)\n",
    "        self.mask = (torch.tril(torch.ones(seq_len, seq_len)) == 0)\n",
    "        self.projection = nn.Linear(nb_heads*head_size, embed_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0) # x: batch_size x seq_len x embed_size\n",
    "        # compute q, k, v\n",
    "        q = self.query(x) # batch_size x seq_len x nb_heads*head_size\n",
    "        q = q.view(batch_size, self.seq_len, self.nb_heads, self.head_size) # batch_size x seq_len x nb_heads x head_size\n",
    "        q = q.permute(0, 2, 1, 3) # batch_size x nb_heads x seq_len x head_size\n",
    "        k = self.key(x).view(batch_size, self.seq_len, self.nb_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        v = self.value(x).view(batch_size, self.seq_len, self.nb_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # compute multi-head attention\n",
    "        att = q @ k.transpose(2, 3) / self.head_size**0.5 # batch_size x nb_heads x seq_len x seq_len\n",
    "        att.masked_fill_(mask=self.mask, value=float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = att @ v # batch_size x nb_heads x seq_len x head_size\n",
    "        \n",
    "        # concatenate heads and project\n",
    "        att = att.permute(0, 2, 1, 3).reshape(batch_size, self.seq_len, self.nb_heads*self.head_size) # batch_size x seq_len x nb_heads*head_size\n",
    "        att = self.projection(att) # batch_size x seq_len x embed_size\n",
    "        return att\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, seq_len, embed_size, nb_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.masked_multi_head_attention = MaskedMultiHeadAttention(seq_len, embed_size, nb_heads, head_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embed_size, embed_size))\n",
    "        self.layer_norm = nn.LayerNorm(embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.masked_multi_head_attention(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class LesMiserablesLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, embed_size, nb_heads, head_size, n_blocks):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block(seq_len, embed_size, nb_heads, head_size) for _ in range(n_blocks)])\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x) + self.position_embedding(torch.arange(seq_len)) # batch_size x seq_len x embedding_dim\n",
    "        x = self.blocks(x) # batch_size x seq_len x embedding_dim\n",
    "        print(x.shape)\n",
    "        x = self.linear(x) # batch_size x seq_len x vocab_size\n",
    "        return x\n",
    "\n",
    "lmlm = LesMiserablesLanguageModel(vocab_size, seq_len, embed_size=64, nb_heads=4, head_size=16, n_blocks=6)\n",
    "x = train_dataset[0][0].unsqueeze(0)\n",
    "lmlm(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
